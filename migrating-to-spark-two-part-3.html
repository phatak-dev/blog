<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Migrating to Spark 2.0 - Part 3 : DataFrame to Dataset</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/migrating-to-spark-two-part-3">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Migrating to Spark 2.0 - Part 3 : DataFrame to Dataset</h1>
  <p class="meta">May 8, 2017</p>
  <div class="catagories">
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/spark-two-migration-series"><span class="category">spark-two-migration-series</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Spark 2.0 brings a significant changes to abstractions and API’s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.</p>

<p>This is the third post in this series.In this post we discuss about migrating  dataframe based API’s to dataset based once. You can access all the posts <a href="/categories/spark-two-migration-series">here</a>.</p>

<p>TL;DR You can access all the code on <a href="https://github.com/phatak-dev/spark-two-migration">github</a>.</p>

<h2 id="dataframe-abstraction-in-spark-1x">DataFrame Abstraction in Spark 1.x</h2>

<p>In 1.x series of spark, dataframe was a structured abstraction over native RDD abstraction. Dataframe 
was built to support the structured transformation of data, using dataframe dsl or spark sql query language.
Where as rdd abstraction was there to provide the functional API’s for manipulating the data.</p>

<p>So whenever we wanted to use functional API’s on dataframe, we would be converting dataframe into a
RDD and then manipulated as RDD abstraction. This kind of conversion made it’s easy to move between dataframe 
and rdd abstractions.</p>

<h2 id="dataframe-to-rdd-example">DataFrame to RDD Example</h2>

<p>The below code shows, how to use functional <em>map</em> API on dataframe abstraction</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">loadedDF</span> <span class="k">=</span> <span class="nv">sqlContext</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"com.databricks.spark.csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">load</span><span class="o">(</span><span class="s">"../test_data/sales.csv"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">amountRDD</span> <span class="k">=</span> <span class="nv">loadedDF</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">row</span> <span class="k">⇒</span> <span class="nv">row</span><span class="o">.</span><span class="py">getDouble</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span></code></pre></figure>

<p>In above code, whenever we call <em>map</em> spark implicitly converts to an RDD. So this is the way we operated on the dataframe in spark 1.x.</p>

<p>You can access complete code on <a href="https://github.com/phatak-dev/spark-two-migration/blob/master/spark-one/src/main/scala/com/madhukaraphatak/spark/migration/sparkone/DFMapExample.scala">github</a>.</p>

<h2 id="dataframe-abstraction-in-2x">DataFrame abstraction in 2.x</h2>

<p>From spark 2.x, the dataframe abstraction has changed significantly. In 2.x, dataframe is alias of Dataset[Row].Dataset is combination of both dataframe and RDD like API’s. So not only dataset supports structured querying using dsl and sql, it also supports the functional API’s which are supported in RDD.</p>

<p>So whenever we call <em>map</em> in 2.x, we no more get a RDD. Instead we get dataset. This change in the conversion will break your code if you are using the RDD based code showed earlier.</p>

<h2 id="porting-dataframe-to-rdd-code-in-2x">Porting DataFrame to RDD code in 2.x</h2>

<p>To port your existing code,you need to add extra, <em>.rdd</em> before calling map method as shown in the code.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">amountRDD</span> <span class="k">=</span> <span class="nv">loadedDF</span><span class="o">.</span><span class="py">rdd</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">row</span> <span class="k">⇒</span> <span class="nv">row</span><span class="o">.</span><span class="py">getDouble</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span>
<span class="nf">println</span><span class="o">(</span><span class="nv">amountRDD</span><span class="o">.</span><span class="py">collect</span><span class="o">.</span><span class="py">toList</span><span class="o">)</span></code></pre></figure>

<h2 id="rdd-to-dataset-abstraction">RDD to Dataset Abstraction</h2>

<p>The above code makes it easy to port but doesn’t provide good performance. Also converting rdd back to dataframe is a tedious work. So rather than using rdd functional API’s, you can use dataset functional API’s. Only constraint is that the return type of map should have a encoder. By default all primitive types and case classes are supported.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">sparkSession.implicits._</span>
<span class="k">val</span> <span class="nv">amountDataSet</span> <span class="k">=</span> <span class="nv">loadedDF</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">row</span> <span class="k">⇒</span> <span class="nv">row</span><span class="o">.</span><span class="py">getDouble</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span></code></pre></figure>

<p>By importing <em>sparkSession.implicits._</em> in to the scope, we are importing all default encoders. As we are returning a double value, there is a built in encoder for the same.</p>

<p>This code is more performant than the RDD code. So use dataset based functional API wherever you were using RDD before. Only fall back to RDD API, whenever dataset API doesn’t support that API.</p>

<p>You can access complete code on <a href="https://github.com/phatak-dev/spark-two-migration/blob/master/spark-two/src/main/scala/com/madhukaraphatak/spark/migration/sparktwo/DFMapExample.scala">github</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we learnt how to port RDD based functional API’s to more peformant dataset alternatives.</p>


</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">26 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-2">Understanding Spark Connect API - Part 2: Introduction to Architecture</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">09 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-1">Understanding Spark Connect API - Part 1: Shortcomings of Spark Driver Architecture</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">26 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-2">Understanding Spark Connect API - Part 2: Introduction to Architecture</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">09 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-1">Understanding Spark Connect API - Part 1: Shortcomings of Spark Driver Architecture</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52311191-1', 'auto');
  ga('send', 'pageview');

</script>

    </body>
</html>