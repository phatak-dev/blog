<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Introduction to Spark Structured Streaming - Part 2 : Source and Sinks</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/introduction-to-spark-structured-streaming-part-2">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Introduction to Spark Structured Streaming - Part 2 : Source and Sinks</h1>
  <p class="meta">Aug 1, 2017</p>
  <div class="catagories">
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/introduction-structured-streaming"><span class="category">introduction-structured-streaming</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Structured Streaming is a new streaming API, introduced in spark 2.0, rethinks stream processing in spark land. It models stream
as an infinite table, rather than discrete collection of data. It’s a radical departure from models of other stream processing frameworks like
storm, beam, flink etc. Structured Streaming is the first API to build stream processing on top of SQL engine.</p>

<p>Structured Streaming was in alpha in 2.0 and 2.1. But with release 2.2 it has hit stable status. In next few releases,
it’s going to be de facto way of doing stream processing in spark. So it will be right time to make ourselves familiarise
with this new API.</p>

<p>In this series of posts, I will be discussing about the different aspects of the structured streaming API. I will be discussing about
new API’s, patterns and abstractions to solve common stream processing tasks.</p>

<p>This is the second post in the series. In this post, we discuss about the source and sink abstractions. You 
can read all the posts in the series <a href="/categories/introduction-structured-streaming">here</a>.</p>

<p>TL;DR You can access code on <a href="https://github.com/phatak-dev/spark2.0-examples/tree/master/src/main/scala/com/madhukaraphatak/examples/sparktwo/streaming">github</a>.</p>

<h2 id="datasource-api">Datasource API</h2>
<p>In spark 1.3, with introduction of DataFrame abstraction, spark has introduced an API to read structured data from variety of sources.
This API is known as datasource API. Datasource API is an universal API to read structured data from different sources like databases, 
csv files etc. The data read from datasource API is represented as DataFrame in the program. So data source API has become de facto way
of creating dataframes in spark batch API.</p>

<p>You can read more about datasource API in my post <a href="/introduction-to-spark-data-source-api-part-1">Introduction to Spark Data Source API</a>.</p>

<h2 id="extending-datasource-api-for-streams">Extending Datasource API for streams</h2>

<p>With Structured Streaming, streaming is moving towards dataframe abstraction. So rather than creating
a new API to create dataframe’s for streaming, spark has extended the datasource API to support stream. It has added a new method <em>readStream</em>
which is similar to <em>read</em> method.</p>

<p>Having same abstraction for reading data in both batch and streaming makes code more consistent and easy to understand.</p>

<h2 id="reading-from-socket-stream">Reading from Socket Stream</h2>

<p>As an example to show case the datasource API, let’s read from socket stream. The below is the code to do that</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala">  <span class="k">val</span> <span class="nv">socketStreamDf</span> <span class="k">=</span> <span class="nv">sparkSession</span><span class="o">.</span><span class="py">readStream</span>
   <span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"socket"</span><span class="o">)</span>
   <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"host"</span><span class="o">,</span> <span class="s">"localhost"</span><span class="o">)</span>
   <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"port"</span><span class="o">,</span> <span class="mi">50050</span><span class="o">)</span>
   <span class="o">.</span><span class="py">load</span><span class="o">()</span></code></pre></figure>

<p>As you can observe from above code, reading a stream has become very similar to reading static data. In the code, <em>readStream</em> indicates
we are reading continuous data rather than static data.</p>

<p>If you have done spark streaming before, you may have observed there is no mention of batch time. This is because in structured streaming,
the rate of consumption of stream is determined by the sink not by the source. So when we create the source we don’t need to worry about the
time information.</p>

<p>The result , <em>socketStreamDf</em> is a dataframe containing data from socket.</p>

<h2 id="schema-inference-in-structured-streaming">Schema Inference in Structured Streaming</h2>

<p>One of the important feature of data source API is it’s support for the schema inference. This means it can go through the data 
to automatically understand the schema and fill that in for dataframe. This is better than specifying the schema manually. But
how that works for streams?</p>

<p>For streams, currently schema inference is not supported. The schema of the data has to be provided by the user or the
data source connector. In our socket stream example, the schema is provided by the socket connector. The schema contains
single column named <em>value</em> of the type <em>string</em>. This column contains the data from socket in string format.</p>

<p>So in structured streaming we will be specifying the schema explicitly contrast to schema inference of batch API.</p>

<h2 id="writing-to-sink">Writing to Sink</h2>

<p>To complete a stream processing, we need both source and sink. We have created dataframe from socket source. Let’s write
that to a sink.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">consoleDataFrameWriter</span> <span class="k">=</span> <span class="nv">socketStreamDf</span><span class="o">.</span><span class="py">writeStream</span>
      <span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"console"</span><span class="o">)</span>
      <span class="o">.</span><span class="py">outputMode</span><span class="o">(</span><span class="nv">OutputMode</span><span class="o">.</span><span class="py">Append</span><span class="o">())</span></code></pre></figure>

<p>To create a sink, we use <em>writeStream</em>. In our example, we are using <em>console</em> sink which just prints the data to the console. In the code,
we have specified output mode, which is similar to save modes in batch API. We will talk more about them in future posts.</p>

<p>The result of <em>writeStream</em> is a <em>DataStreamWriter</em>. Now we have connected the source and sink.</p>

<h2 id="streaming-query-abstraction">Streaming Query Abstraction</h2>

<p>Once we have connected the source and sink, next step is to create a streaming query. <em>StreamingQuery</em> is an abstraction for query that is executing continuously in the background as new data arrives. This abstraction is the entry point for starting the stream processing. All the steps before it was the setting up the stream computatiuons.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">query</span> <span class="k">=</span> <span class="nv">consoleDataFrameWriter</span><span class="o">.</span><span class="py">start</span><span class="o">()</span></code></pre></figure>

<p>Above code creates a streaming query from the dataframe writer. Once we have the query object, we can run <em>awaitTermination</em> to keep it running.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">query</span><span class="o">.</span><span class="py">awaitTermination</span><span class="o">()</span></code></pre></figure>

<p>You can access complete code on <a href="https://github.com/phatak-dev/spark2.0-examples/blob/master/src/main/scala/com/madhukaraphatak/examples/sparktwo/streaming/SocketReadExample.scala">github</a>.</p>

<h2 id="running-the-example">Running the example</h2>

<p>Before we can run the example, we need to start the socket stream. You can do that by running below command</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">nc <span class="nt">-lk</span> localhost 50050</code></pre></figure>

<p>Now you can enter data in stdin, you can observe the result in the console.</p>

<p>We have successfully ran our first structured streaming example.</p>

<h2 id="batch-time">Batch Time</h2>

<p>After successfully running the example, one question immediately comes in to mind. How frequently socket data is processed?. Another way of asking question is,
what’s the batch time and where we have specified in code?</p>

<p>In structured streaming, there is no batch time. Rather than batch time, we use trigger abstraction to indicate the frequency of processing. Triggers can be
specified in variety of ways. One of the way of specifying is using processing time which is similar to batch time of earlier API.</p>

<p>By default, the trigger is <em>ProcessingTime(0)</em>. Here 0 indicates asap. This means as and when data arrives from the source spark tries to process it. This is very
similar to per message semantics of the other streaming systems like storm.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Structured Streaming defines source and sinks using data source API. This unification of API makes it easy to move from batch world to
streaming world.</p>

</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0ZF0EGSMTQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0ZF0EGSMTQ');
</script>


    </body>
</html>