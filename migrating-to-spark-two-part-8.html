<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Migrating to Spark 2.0 - Part 8 : Catalog API</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/migrating-to-spark-two-part-8">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Migrating to Spark 2.0 - Part 8 : Catalog API</h1>
  <p class="meta">Jun 20, 2017</p>
  <div class="catagories">
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/spark-two-migration-series"><span class="category">spark-two-migration-series</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Spark 2.0 brings a significant changes to abstractions and API’s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.</p>

<p>This is the eighth post in this series.In this post we will discuss about catalog support in spark sql. You can access all the posts <a href="/categories/spark-two-migration-series">here</a>.</p>

<p>TL;DR You can access all the code on <a href="https://github.com/phatak-dev/spark-two-migration">github</a>.</p>

<h2 id="catalog-api">Catalog API</h2>

<p>In spark 1.x, spark heavily depended  on hive for all metastore related operations. Even though sqlContext supported few of the DDL operations, most of them
were very basic and not complete. So spark documentation often recommended using HiveContext over SQLContext. Also whenever user uses HiveContext, spark
support for interacting with hive metastore was limited. So most of the metastore operation’s often done as embedded hive queries.</p>

<p>Spark 2.x changes all of this. It has exposed a full fledged user facing catalog API which works for both spark SQL and hive. Not only it supports
the spark 1.x operations, it has added many new ones to improve the interaction with metastore.</p>

<p>In below sections, we will be discussing about porting earlier metastore operations to new catalog API.</p>

<h2 id="creating-table">Creating Table</h2>

<p>Before we do any DDL operations, we need to create a table. For our example, we will use temporary tables.</p>

<h3 id="spark-1x">Spark 1.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">loadedDf</span> <span class="k">=</span> <span class="nv">sqlContext</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"com.databricks.spark.csv"</span><span class="o">).</span>
  <span class="nf">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">load</span><span class="o">(</span><span class="s">"../test_data/sales.csv"</span><span class="o">)</span>

<span class="nv">loadedDf</span><span class="o">.</span><span class="py">registerTempTable</span><span class="o">(</span><span class="s">"sales"</span><span class="o">)</span></code></pre></figure>

<p>We use <em>registerTempTable</em> API for creating table in in-memory catalog.</p>

<h3 id="spark-2x">Spark 2.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">loadedDf</span> <span class="k">=</span> <span class="nv">sparkSession</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">load</span><span class="o">(</span><span class="s">"../test_data/sales.csv"</span><span class="o">)</span>

<span class="nv">loadedDf</span><span class="o">.</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"sales"</span><span class="o">)</span></code></pre></figure>

<p>In 2.x, <em>registerTempTable</em> API is deprecated. We should use <em>createOrReplaceTempView</em> for the same.</p>

<h2 id="list-tables">List Tables</h2>

<p>Once we have table registered, first catalog operation is listing tables.</p>

<h3 id="spark-1x-1">Spark 1.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sqlContext</span><span class="o">.</span><span class="py">tables</span><span class="o">.</span><span class="py">show</span><span class="o">()</span> </code></pre></figure>

<p>In spark 1.x, catalog API’s were added to context directly. The below is the output</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
|    sales|       true|
+---------+-----------+</code></pre></figure>

<p>In output, it specifies the name of the table and is it temporary or not. When we run same operation on hive metastore, <em>isTemporary</em> will be false.</p>

<h3 id="spark-2x-1">Spark 2.x</h3>

<p>In spark 2.x, there is separate API called <em>catalog</em> on spark session. It exposes all needed API’s.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">listTables</span><span class="o">.</span><span class="py">show</span><span class="o">()</span></code></pre></figure>

<p>The output looks like below</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">+-----+--------+-----------+---------+-----------+
| name|database|description|tableType|isTemporary|
+-----+--------+-----------+---------+-----------+
|sales|    null|       null|TEMPORARY|       true|
+-----+--------+-----------+---------+-----------+</code></pre></figure>

<p>From output it is apparent that output of new catalog API is more richer than old one. Also from spark 2.x, it has added database information as
part of the catalog which was missing in earlier API’s.</p>

<h3 id="list-table-names">List Table Names</h3>

<p>In spark 1.x, there is a API for listing just the name of the tables. The code looks below</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sqlContext</span><span class="o">.</span><span class="py">tableNames</span><span class="o">()</span></code></pre></figure>

<p>In spark 2.x, there is no separate API for getting database names. As listTables, returns a Dataset we can use normal spark sql API’s for getting the name.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">listTables</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"name"</span><span class="o">).</span><span class="py">collect</span></code></pre></figure>

<h2 id="caching">Caching</h2>

<p>As part of the catalog API, we can check is given table is cached or not.</p>

<h3 id="spark-1x-2">Spark 1.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">println</span><span class="o">(</span><span class="nv">sqlContext</span><span class="o">.</span><span class="py">isCached</span><span class="o">(</span><span class="s">"sales"</span><span class="o">))</span></code></pre></figure>

<h3 id="spark-2x-2">Spark 2.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">println</span><span class="o">(</span><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">isCached</span><span class="o">(</span><span class="s">"sales"</span><span class="o">))</span></code></pre></figure>

<h3 id="external-tables">External Tables</h3>

<p>Till now, we worked with tables which we creating using dataframes. Let’s say we need to create table directly from a file without going through data source API. These often known as external tables.</p>

<h3 id="spark-1x-3">Spark 1.x</h3>

<p>In spark 1.x, SQLContext didn’t support creating external tables. So we need to use hivecontext for do that.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">hiveContext</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HiveContext</span><span class="o">(</span><span class="n">sparkContext</span><span class="o">)</span>
 <span class="nv">hiveContext</span><span class="o">.</span><span class="py">setConf</span><span class="o">(</span><span class="s">"hive.metastore.warehouse.dir"</span><span class="o">,</span> <span class="s">"/tmp"</span><span class="o">)</span>
 <span class="nv">hiveContext</span><span class="o">.</span><span class="py">createExternalTable</span><span class="o">(</span><span class="s">"sales_external"</span><span class="o">,</span> <span class="s">"com.databricks.spark.csv"</span><span class="o">,</span> <span class="nc">Map</span><span class="o">(</span>
   <span class="s">"path"</span> <span class="o">-&gt;</span> <span class="s">"../test_data/sales.csv"</span><span class="o">,</span>
   <span class="s">"header"</span> <span class="o">-&gt;</span> <span class="s">"true"</span><span class="o">))</span>
 <span class="nv">hiveContext</span><span class="o">.</span><span class="py">table</span><span class="o">(</span><span class="s">"sales_external"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span></code></pre></figure>

<p>In above code, first we create <em>HiveContext</em>. Then we need to set the warehouse directory so hive context knows where to keep the data. Then we use
<em>createExternalTable</em> API to load the data to <em>sales_external</em> table.</p>

<h3 id="spark-2x-3">Spark 2.x</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">createExternalTable</span><span class="o">(</span><span class="s">"sales_external"</span><span class="o">,</span> <span class="s">"com.databricks.spark.csv"</span><span class="o">,</span> <span class="nc">Map</span><span class="o">(</span>
 <span class="s">"path"</span> <span class="o">-&gt;</span> <span class="s">"../test_data/sales.csv"</span><span class="o">,</span>
 <span class="s">"header"</span> <span class="o">-&gt;</span> <span class="s">"true"</span><span class="o">))</span>
<span class="nv">sparkSession</span><span class="o">.</span><span class="py">table</span><span class="o">(</span><span class="s">"sales_external"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span></code></pre></figure>

<p>In spark 2.x, creating external table is part of catalog API itself. We don’t need to enable hive for this functionality.</p>

<h2 id="additional-apis">Additional API’s</h2>

<p>The above examples showed the porting of spark 1.x code to spark 2.x. But there are additional API’s in spark 2.x catalog which are useful
in day to day development. Below sections discusses few of them.</p>

<h2 id="list-functions">List Functions</h2>

<p>The below code list all the functions, built in and user defined. It helps us to know what are the Udfs, Udaf’s available in current session.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">listFunctions</span><span class="o">.</span><span class="py">show</span><span class="o">()</span></code></pre></figure>

<p>The sample output looks as below</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">+--------------------+--------+-----------+--------------------+-----------+
|                name|database|description|           className|isTemporary|
+--------------------+--------+-----------+--------------------+-----------+
|                   !|    null|       null|org.apache.spark....|       true|
|                   %|    null|       null|org.apache.spark....|       true|
|                   &amp;|    null|       null|org.apache.spark....|       true|
|                acos|    null|       null|org.apache.spark....|       true|
|          add_months|    null|       null|org.apache.spark....|       true|
|                 and|    null|       null|org.apache.spark....|       true|
|approx_count_dist...|    null|       null|org.apache.spark....|       true|
+--------------------+--------+-----------+--------------------+-----------+</code></pre></figure>

<h2 id="list-columns">List Columns</h2>

<p>We can also list the columns of a table.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">sparkSession</span><span class="o">.</span><span class="py">catalog</span><span class="o">.</span><span class="py">listColumns</span><span class="o">(</span><span class="s">"sales"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span></code></pre></figure>

<p>The below is the output for our <em>sales</em> table.</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">+-------------+-----------+--------+--------+-----------+--------+
|         name|description|dataType|nullable|isPartition|isBucket|
+-------------+-----------+--------+--------+-----------+--------+
|transactionId|       null|  string|    true|      false|   false|
|   customerId|       null|  string|    true|      false|   false|
|       itemId|       null|  string|    true|      false|   false|
|   amountPaid|       null|  string|    true|      false|   false|
+-------------+-----------+--------+--------+-----------+--------+</code></pre></figure>

<h2 id="complete-code">Complete code</h2>

<p>You can access complete code for spark 1.x <a href="https://github.com/phatak-dev/spark-two-migration/blob/master/spark-one/src/main/scala/com/madhukaraphatak/spark/migration/sparkone/CatalogExample.scala">here</a>.</p>

<p>You can access complete code for spark 2.x <a href="https://github.com/phatak-dev/spark-two-migration/blob/master/spark-two/src/main/scala/com/madhukaraphatak/spark/migration/sparktwo/CatalogExample.scala">here</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog we have discussed about improvements in catalog API. Using new catalog API, you can get information of tables much easier than before.</p>

</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0ZF0EGSMTQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0ZF0EGSMTQ');
</script>


    </body>
</html>