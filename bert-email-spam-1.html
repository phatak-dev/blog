<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Email Spam Detection using Pre-Trained BERT Model : Part 1 - Introduction and Tokenization</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/bert-email-spam-1">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Email Spam Detection using Pre-Trained BERT Model : Part 1 - Introduction and Tokenization</h1>
  <p class="meta">Feb 13, 2023</p>
  <div class="catagories">
    
    <a href="/categories/python"><span class="category">python</span></a>
    
    <a href="/categories/transformer-models"><span class="category">transformer-models</span></a>
    
    <a href="/categories/bert-email-spam"><span class="category">bert-email-spam</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Recently I have been looking into Transformer based machine learning models for natural language tasks. The field of NLP has changed tremendously in last few years and I have been fascinated by the new architectures and tools that they are come out in same time. Transformer models is one of such architecture.</p>

<p>As the frameworks and tools to build transformer models keeps evolving, the documentation often become stale and blog posts often confusing. So for any one topic, you may find multiple approaches which can confuse beginner.</p>

<p>So as I am learning these models, I am planning to document the steps to do few of the important tasks in simplest way possible. This should help any beginner like me to pickup transformer models.</p>

<p>In this two part series, I will be discussing about  how to train a simple model for email spam classification using pre trained transformer BERT model.This is the first post in series where I will be discussing about transformer models and preparing our data. You can read all the posts in the series <a href="/categories/bert-email-spam">here</a>.</p>

<h2 id="transformer-models">Transformer Models</h2>

<p>Transformer is a neural network architecture first introduced by Google in 2017. This architecture has proven extremely efficient in learning various tasks. Some of the popular models of transformer architecture is BERT, Distilbert, GPT-3, chatGPT etc.</p>

<p>You can read more about transformer models in below link</p>

<p><a href="https://huggingface.co/course/chapter1/4">https://huggingface.co/course/chapter1/4</a>.</p>

<h2 id="pre-trained-language-model-and-transfer-learning">Pre-Trained Language Model and Transfer Learning</h2>

<p>A pre-trained language model is a transformer model, which is trained on large amount of language data for specific tasks.</p>

<p>The idea behind using pre-trained model is that, model has really good understand of language which we can borrow for our nlp task as it is and just focus on training unique part of task in our model. This is called as transfer learning. You can read more about transfer learning in below link</p>

<p><a href="https://huggingface.co/course/chapter1/4#transfer-learning">https://huggingface.co/course/chapter1/4#transfer-learning</a>.</p>

<h2 id="google-colab">Google Colab</h2>

<p>Google Colab is a hosted jupyter python notebook which has access GPU runtime. As these transformer models perform extremely well on GPU, we are going to use google colab for our examples. You can get community version of same by signing in using your google credentials.</p>

<h2 id="installing-libraries">Installing Libraries</h2>

<p>First step to install libraries. These libraries come from huggingface, a company that provides tools for simplifying building transformer based models.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">evaluate</span></code></pre></figure>

<p>In above</p>

<ul>
  <li><em>transformer</em> library provides all the pre trained models and tools to train a model</li>
  <li><em>datasets</em> library provides tool to load and use datasets in form required by above models</li>
  <li><em>evaluate</em> a helper library to calculate metrics for training</li>
</ul>

<h2 id="email-spam-data-and-preparation">Email Spam Data and Preparation</h2>

<p>In this section of the post, we will be discussing about our spam data and it’s preparation.</p>

<h3 id="1-spam-data">1. Spam Data</h3>

<p>For our example, we are going to use the email spam data from below link</p>

<p><a href="https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv">https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv</a></p>

<p>The data has two important fields</p>

<ol>
  <li>v2 - Content of Email</li>
  <li>v1 - Label which indicates spam or not</li>
</ol>

<p><strong>Please download data from kaggle and upload to your instance of google colab.</strong></p>

<h3 id="2-loading-data-to-dataframe">2. Loading Data to Dataframe</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="c1"># data from https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv
</span><span class="n">file_path</span><span class="o">=</span><span class="s">"spam.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span><span class="n">encoding</span> <span class="o">=</span> <span class="s">"ISO-8859-1"</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">"v1"</span><span class="p">,</span><span class="s">"v2"</span><span class="p">]]</span></code></pre></figure>

<h3 id="3-mapping-the-labels">3. Mapping the Labels</h3>

<p>In the data, labels are “ham” and “spam”. We need to map them to 0 and 1. The below code does the same.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="nb">map</span><span class="p">({</span><span class="s">'ham'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">'spam'</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span></code></pre></figure>

<h3 id="4-generating-different-datasets">4. Generating Different Datasets</h3>

<p>Once we have mapped labels, we will be creating train, test and validate sets.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="n">train_data_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">eval_data_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">train_data_df</span><span class="p">)</span>
<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">eval_data_df</span><span class="p">)</span>
<span class="n">test_data_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4000</span><span class="p">:</span><span class="mi">4100</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">test_data_df</span><span class="p">)</span></code></pre></figure>

<p><strong>In the above code, we use the <em>Dataset.from_pandas</em> to create hugging face compatible datasets which will be using in next steps.</strong></p>

<h2 id="tokenization">Tokenization</h2>

<p>To use any pre-trained model, one of the pre requisites is that we need to use tokenization of the model on our dataset. This will make sure that the model can take our data as input.</p>

<h3 id="1-download-tokenizer">1. Download Tokenizer</h3>

<p>First step in the tokenization is to download right tokenization model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"bert-base-uncased"</span><span class="p">)</span></code></pre></figure>

<p>Hugging face transformer library provides the helper class called AutoTokenizer. This class provides method <em>from_pretrained</em> which will help to download the tokenization model from hugging face repository. The model we are using base bert model trained on uncased data.</p>

<h3 id="2-tokenize-datasets">2. Tokenize Datasets</h3>

<p>Once tokenizer is downloaded and ready to use, we can tokenize our datasets.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"v2"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">tokenized_datasets_train</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokenized_datasets_eval</span> <span class="o">=</span> <span class="n">eval_dataset</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokenizer_datasets_test</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p>In above code, we use <em>tokenize_function</em> which selects the right column which has the text data. Then using the <em>map</em> function tokenization will be applied for each batch.</p>

<h2 id="code">Code</h2>

<p>Complete code for the post is in below google colab notebook.</p>

<p><a href="https://colab.research.google.com/drive/1d3-22HNkuLCqP1ctemMaMw1ETaWL48ET?usp=sharing">https://colab.research.google.com/drive/1d3-22HNkuLCqP1ctemMaMw1ETaWL48ET?usp=sharing</a>.</p>

<p>You can also access python notebook on github.</p>

<p><a href="https://github.com/phatak-dev/transformer-models/blob/main/Spam_Detection_using_BERT_Model.ipynb">https://github.com/phatak-dev/transformer-models/blob/main/Spam_Detection_using_BERT_Model.ipynb</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we understood what are transformation models. We also prepared our dataset to have model tokenization. In the next post, we will see how to fine tune the model.</p>

</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">16 Feb 2023</span>
     &raquo; <a href="/bert-email-spam-2">Email Spam Detection using Pre-Trained BERT Model : Part 2 - Model Fine Tuning</a>    
   </li>           
         

            
          

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0ZF0EGSMTQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0ZF0EGSMTQ');
</script>


    </body>
</html>