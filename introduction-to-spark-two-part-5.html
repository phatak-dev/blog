<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Introduction to Spark 2.0 - Part 5 : Time Window in Spark SQL</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/introduction-to-spark-two-part-5">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Introduction to Spark 2.0 - Part 5 : Time Window in Spark SQL</h1>
  <p class="meta">May 19, 2016</p>
  <div class="catagories">
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/spark-two"><span class="category">spark-two</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Spark 2.0 is the next major release of Apache Spark. This release brings major changes to abstractions, API’s and libraries of the platform. This release sets the tone for next year’s direction of the framework. So understanding these few features is critical to understand for the ones who want to make use all the advances in this new release. So in this series of blog posts, I will be discussing about different improvements landing in Spark 2.0.</p>

<p>This is the fifth blog in series, where I will be discussing about time window API. You can access all the posts in the series <a href="/categories/spark-two/">here</a>.</p>

<p>TL;DR All code examples are available on <a href="https://github.com/phatak-dev/spark2.0-examples">github</a>.</p>

<h2 id="window-api-in-spark-sql">Window API in Spark SQL</h2>

<p>Spark introduced window API in 1.4 version to support smarter grouping functionalities. They are very useful for people coming from SQL background. One of the missing window API was ability to create windows using time. Time plays an important role in many industries like finance, telecommunication where understanding the data depending upon the time becomes crucial.</p>

<p>In Spark 2.0, framework has introduced built in support for time windows. These behave very similar to time windows in spark-streaming. In this blog post, I will be discussing about how to use this time window API.</p>

<h2 id="time-series-data">Time Series Data</h2>

<p>Before we start doing time window, we need to have access to a time series data. For my example, I will be using data of Apple stock from 1980 to 2016. You can access the data <a href="https://raw.githubusercontent.com/phatak-dev/spark2.0-examples/master/src/main/scala/com/madhukaraphatak/examples/sparktwo/TimeWindowExample.scala">here</a>. The original source of data is <a href="https://in.finance.yahoo.com/q/hp?s=AAPL">yahoo finance</a>.</p>

<p>The data has six columns. Out of those six, we are only interested in <em>Date</em>, which signifies the date of trade and <em>Close</em> which signifies end of the day value.</p>

<h2 id="importing-time-series-data-to-dataframe">Importing time series data to DataFrame</h2>

<p>Once we have time series data, we need to import it to dataframe. All the time window API’s need a column with type timestamp. Luckily <em>spark-csv</em> package can automatically infer the date formats from data and create schema accordingly. The below code is for importing with schema inference.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala">  
  <span class="k">val</span> <span class="nv">stocksDF</span> <span class="k">=</span> <span class="nv">sparkSession</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span><span class="s">"true"</span><span class="o">).</span>
      <span class="nf">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span><span class="s">"true"</span><span class="o">)</span>
      <span class="o">.</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/main/resources/applestock.csv"</span><span class="o">)</span>    </code></pre></figure>

<h2 id="find-weekly-average-in-2016">Find weekly average in 2016</h2>

<p>Once we have data is represented as dataframe, we can start doing time window analysis. In our analysis, we want to find weekly average of the stock for 2016. The below are the steps to do that.</p>

<h3 id="step-1--filter-data-for-2016">Step 1 : Filter data for 2016</h3>

<p>As we are interested only in 2016, we need to filter the data for 2016. The below code show how to filter data on time.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala">  
 <span class="k">val</span> <span class="nv">stocks2016</span> <span class="k">=</span> <span class="nv">stocksDF</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="s">"year(Date)==2016"</span><span class="o">)</span></code></pre></figure>

<p>We can use builtin function year, as Date is already represented as a timestamp.</p>

<h3 id="step-2--tumbling-window-to-calculate-average">Step 2 : Tumbling window to calculate average</h3>

<p>Once we have filtered data, we need to create window for every 1 week. This kind of discretization of data is called as a tumbling window.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala">  
 <span class="k">val</span> <span class="nv">tumblingWindowDS</span> <span class="k">=</span> <span class="n">stocks2016</span>
      <span class="o">.</span><span class="py">groupBy</span><span class="o">(</span><span class="nf">window</span><span class="o">(</span><span class="nv">stocks2016</span><span class="o">.</span><span class="py">col</span><span class="o">(</span><span class="s">"Date"</span><span class="o">),</span><span class="s">"1 week"</span><span class="o">))</span>
      <span class="o">.</span><span class="py">agg</span><span class="o">(</span><span class="nf">avg</span><span class="o">(</span><span class="s">"Close"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"weekly_average"</span><span class="o">))</span></code></pre></figure>

<p>The above code show how to use time window API. Window is normally used inside a group by. The first parameter signifies which column needs to be treated as time. Second parameter signifies the window duration. Window duration can be seconds, minutes, hours, days or weeks.</p>

<p>Once we have created window, we can run an aggregation like average as shown in the code.</p>

<h3 id="step-3--printing-the-window-values">Step 3 : Printing the window values</h3>

<p>Once we calculated the time window, we want to see the result.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">printWindow</span><span class="o">(</span><span class="n">tumblingWindowDS</span><span class="o">,</span><span class="s">"weekly_average"</span><span class="o">)</span></code></pre></figure>

<p>The above code uses a helper function called <em>printWindow</em> which takes aggregated window dataframe and aggregated column name. The helper function looks as follows.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="nf">printWindow</span><span class="o">(</span><span class="n">windowDF</span><span class="k">:</span><span class="kt">DataFrame</span><span class="o">,</span> <span class="n">aggCol</span><span class="k">:</span><span class="kt">String</span><span class="o">)</span> <span class="o">={</span>
    <span class="nv">windowDF</span><span class="o">.</span><span class="py">sort</span><span class="o">(</span><span class="s">"window.start"</span><span class="o">).</span>
    <span class="nf">select</span><span class="o">(</span><span class="s">"window.start"</span><span class="o">,</span><span class="s">"window.end"</span><span class="o">,</span><span class="n">s</span><span class="s">"$aggCol"</span><span class="o">).</span>
    <span class="nf">show</span><span class="o">(</span><span class="n">truncate</span> <span class="k">=</span> <span class="kc">false</span><span class="o">)</span>
 <span class="o">}</span></code></pre></figure>

<p>In above function, we are sorting dataframe using <em>window.start</em>. This column signifies the start time of window. This sorting helps us to understand the output better. Once we have sorted, we print start,end, aggregated value. As the timestamp can be long, we tell the show not to truncate results for better display.</p>

<p>When you run the example, we see the below result.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+---------------------+------------------+
|start                |end                  |weekly_average    |
+---------------------+---------------------+------------------+
|2015-12-31 05:30:00.0|2016-01-07 05:30:00.0|101.30249774999999|
|2016-01-07 05:30:00.0|2016-01-14 05:30:00.0|98.47199859999999 |
|2016-01-14 05:30:00.0|2016-01-21 05:30:00.0|96.72000125000001 |
|2016-01-21 05:30:00.0|2016-01-28 05:30:00.0|97.6719984        |

</code></pre></div></div>

<p>One thing you may observe is the date is started from 31st and first week is considered till 7. But if you go through the data, the first entry for 2016 start from 2016-01-04. The reason is there was no trading on 1st as it’s new year, 2 and 3 as they are weekend.</p>

<p>We can fix this by specifying the start time for window, which signifies the offset from which window should start.</p>

<h2 id="time-window-with-start-time">Time window with start time</h2>

<p>In earlier code, we used a tumbling window. In order to specify start time we need to use a sliding window. As of now, there is no API which combines tumbling window with start time. We can create tumbling window effect by keeping both window duration and slide duration same.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">windowWithStartTime</span> <span class="k">=</span> <span class="nv">stocks2016</span><span class="o">.</span><span class="py">groupBy</span><span class="o">(</span><span class="nf">window</span><span class="o">(</span><span class="nv">stocks2016</span><span class="o">.</span><span class="py">col</span><span class="o">(</span><span class="s">"Date"</span><span class="o">),</span>
                          <span class="s">"1 week"</span><span class="o">,</span><span class="s">"1 week"</span><span class="o">,</span> <span class="s">"4 days"</span><span class="o">))</span>
                          <span class="o">.</span><span class="py">agg</span><span class="o">(</span><span class="nf">avg</span><span class="o">(</span><span class="s">"Close"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"weekly_average"</span><span class="o">))</span></code></pre></figure>

<p>In above code, we specify “4 days” which is a offset for start time. The first two parameters specify window duration and slide duration.When we run this code, we observe the below result</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+---------------------+------------------+
|start                |end                  |weekly_average    |
+---------------------+---------------------+------------------+
|2015-12-28 05:30:00.0|2016-01-04 05:30:00.0|105.349998        |
|2016-01-04 05:30:00.0|2016-01-11 05:30:00.0|99.0699982        |
|2016-01-11 05:30:00.0|2016-01-18 05:30:00.0|98.49999799999999 |
|2016-01-18 05:30:00.0|2016-01-25 05:30:00.0|98.1220016        |
</code></pre></div></div>

<p>Now we have a week starting from <em>2016-01-04</em>. Still we have initial row which is take from 2015. The reason is, as our start time is 4 days, it creates a window till that time from last seven days.We can remove this row easily using filter as below.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">filteredWindow</span> <span class="k">=</span> <span class="nv">windowWithStartTime</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="s">"year(window.start)=2016"</span><span class="o">)</span></code></pre></figure>

<p>Now we will see the expected result.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+---------------------+------------------+
|start                |end                  |weekly_average    |
+---------------------+---------------------+------------------+
|2016-01-04 05:30:00.0|2016-01-11 05:30:00.0|99.0699982        |
|2016-01-11 05:30:00.0|2016-01-18 05:30:00.0|98.49999799999999 |
|2016-01-18 05:30:00.0|2016-01-25 05:30:00.0|98.1220016        |
|2016-01-25 05:30:00.0|2016-02-01 05:30:00.0|96.2539976        |
|2016-02-01 05:30:00.0|2016-02-08 05:30:00.0|95.29199960000001 |
</code></pre></div></div>

<p>You can access complete code <a href="https://github.com/phatak-dev/spark2.0-examples/blob/master/src/main/scala/com/madhukaraphatak/examples/sparktwo/TimeWindowExample.scala">here</a>.</p>

<p>So now we know how to use time windows in Spark 2.0. This is one of the powerful feature which helps in wide variety analysis in big data.</p>


</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">14 Sep 2022</span>
     &raquo; <a href="/latest-java-2">Latest Java Features from a Scala Dev Perspective - Part 2: Lambda Expressions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">14 Sep 2022</span>
     &raquo; <a href="/latest-java-1">Latest Java Features from a Scala Dev Perspective - Part 1: Type Inference</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">02 Nov 2020</span>
     &raquo; <a href="/spark-3-introduction-part-10">Introduction to Spark 3.0 - Part 10 : Ignoring Data Locality in Spark</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">21 Jul 2021</span>
     &raquo; <a href="/spark-pandas-part-2">Pandas API on Apache Spark - Part 2: Hello World</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">21 Jul 2021</span>
     &raquo; <a href="/spark-pandas-part-1">Pandas API on Apache Spark - Part 1: Introduction</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">11 Nov 2020</span>
     &raquo; <a href="/barrier-execution-mode-part-2">Barrier Execution Mode in Spark 3.0 - Part 2 : Barrier RDD</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52311191-1', 'auto');
  ga('send', 'pageview');

</script>

    </body>
</html>