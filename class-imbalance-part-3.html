<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Class Imbalance in Credit Card Fraud Detection - Part 3 : Undersampling in Spark</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/class-imbalance-part-3">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Class Imbalance in Credit Card Fraud Detection - Part 3 : Undersampling in Spark</h1>
  <p class="meta">Jan 8, 2018</p>
  <div class="catagories">
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/datascience"><span class="category">datascience</span></a>
    
    <a href="/categories/class-imbalance"><span class="category">class-imbalance</span></a>
    
    <a href="/categories/python"><span class="category">python</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Whenever we do classification in ML, we often assume that target label is evenly distributed in our dataset. This helps the training algorithm to learn the features as we have enough examples for all the different cases. For example, in learning a spam filter, we should have good amount of data which corresponds to emails which are spam and non spam.</p>

<p>This even distribution is not always possible. Let’s take an example of fraud detection. Fraud detection is a use case, where by looking at transaction we need to decide is the transaction is fraudulent or not. In majority of the cases, the transaction will be normal. So the data for fraudulent data is very small compared to normal ones. In these cases, there will be imbalance in target labels. This will effect the quality of models we can build.So in next series of posts we will discuss about what’s class imbalance and how to handle it in python and spark.</p>

<p>This is the third post in the series where we discuss about implementing undersampling technique in spark. You can read all the posts in the series <a href="/categories/class-imbalance">here</a>.</p>

<h2 id="spark-dataframe-vs-panda-dataframe">Spark Dataframe vs Panda Dataframe</h2>

<p>In last <a href="/class-imbalance-part-2">post</a> we discussed about how to implement undersampling using panda dataframe. In that code, we used indexes to extract random sample of the non fraud records.</p>

<p>But spark dataframe doesn’t have concept of indexes. As spark dataframe is based on distributed RDD’s, so maintaining order of rows is tricky. So by default, spark dataframe doesn’t preserve any indexes or ordering information. If you need it, you need to explicitly sort it.</p>

<p>So to implement the undersampling in spark, rather than using index technique, we will use sample and union API’s. The below section will explain the code to the same.</p>

<h2 id="undersampling-in-spark">Undersampling in Spark</h2>

<p>The below is the code to do the undersampling in spark</p>

<h3 id="1-find-records-which-are-fraud">1. Find records which are Fraud</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">fraudDf</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="s">"Class=1.0"</span><span class="o">)</span></code></pre></figure>

<h3 id="2-find-records-which-are-non-fraud">2. Find records which are non Fraud</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">nonFraudDf</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="s">"Class=0.0"</span><span class="o">)</span></code></pre></figure>

<h3 id="3-random-sample-non-fraud-records">3. Random sample non fraud records</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">sampleRatio</span> <span class="k">=</span> <span class="nv">fraudDf</span><span class="o">.</span><span class="py">count</span><span class="o">().</span><span class="py">toDouble</span> <span class="o">/</span> <span class="nv">df</span><span class="o">.</span><span class="py">count</span><span class="o">().</span><span class="py">toDouble</span>
<span class="k">val</span> <span class="nv">nonFraudSampleDf</span> <span class="k">=</span> <span class="nv">nonFraudDf</span><span class="o">.</span><span class="py">sample</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="n">sampleRatio</span><span class="o">)</span></code></pre></figure>

<h3 id="4-union-sample-non-fraud-with-fraud-ones">4. Union sample non fraud with fraud ones</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">fraudDf</span><span class="o">.</span><span class="py">unionAll</span><span class="o">(</span><span class="n">nonFraudSampleDf</span><span class="o">)</span></code></pre></figure>

<h2 id="running-logistic-regression-on-undersampled-data">Running Logistic Regression on Undersampled Data</h2>

<p>Once we have undersampled data, we need to train on that.</p>

<h3 id="training">Training</h3>

<p>The below code runs logistic regression on undersampled data. The below code spark ML pipeline API’s.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">amountVectorAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">VectorAssembler</span><span class="o">().</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"Amount"</span><span class="o">)).</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"Amount_vector"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">standarScaler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StandardScaler</span><span class="o">().</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"Amount_vector"</span><span class="o">).</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"Amount_scaled"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">dropColumns</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">"Time"</span><span class="o">,</span><span class="s">"Amount"</span><span class="o">,</span><span class="s">"Class"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">cols</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">columns</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span> <span class="n">column</span> <span class="k">=&gt;</span> <span class="o">!</span><span class="nv">dropColumns</span><span class="o">.</span><span class="py">contains</span><span class="o">(</span><span class="n">column</span><span class="o">))</span> <span class="o">++</span> <span class="nc">Array</span><span class="o">(</span><span class="s">"Amount_scaled"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">vectorAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">VectorAssembler</span><span class="o">().</span><span class="py">setInputCols</span><span class="o">(</span><span class="n">cols</span><span class="o">).</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"features"</span><span class="o">)</span>

<span class="c1">// pipeline</span>
<span class="k">val</span> <span class="nv">logisticRegression</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">LogisticRegression</span><span class="o">().</span><span class="py">setLabelCol</span><span class="o">(</span><span class="s">"Class"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">trainPipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">amountVectorAssembler</span><span class="o">,</span>
<span class="n">standarScaler</span><span class="o">,</span><span class="n">vectorAssembler</span><span class="o">,</span><span class="n">logisticRegression</span><span class="o">))</span>

<span class="k">val</span> <span class="nv">underSampledDf</span> <span class="k">=</span> <span class="nf">underSampleDf</span><span class="o">(</span><span class="n">df</span><span class="o">)</span>
<span class="nf">println</span><span class="o">(</span><span class="s">"for balanced data"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">balancedModel</span> <span class="k">=</span> <span class="nf">runPipeline</span><span class="o">(</span><span class="n">trainPipeline</span><span class="o">,</span> <span class="n">underSampledDf</span><span class="o">)</span></code></pre></figure>

<p>You can access complete code on <a href="https://github.com/phatak-dev/spark-ml-kaggle/blob/master/src/main/scala/com/madhukaraphatak/spark/ml/UnderSampling.scala">github</a>.</p>

<h3 id="results">Results</h3>

<p>Once we run trained the model, we can verify the model using accuracy and recall scores.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">println</span><span class="o">(</span><span class="s">"test accuracy with pipeline "</span> <span class="o">+</span> <span class="nf">accuracyScore</span><span class="o">(</span><span class="nv">model</span><span class="o">.</span><span class="py">transform</span><span class="o">(</span><span class="n">df</span><span class="o">),</span> <span class="s">"Class"</span><span class="o">,</span> <span class="s">"prediction"</span><span class="o">))</span>
<span class="nf">println</span><span class="o">(</span><span class="s">"test recall for 1.0 is "</span> <span class="o">+</span> <span class="nf">recall</span><span class="o">(</span><span class="nv">model</span><span class="o">.</span><span class="py">transform</span><span class="o">(</span><span class="n">df</span><span class="o">),</span> <span class="s">"Class"</span><span class="o">,</span> <span class="s">"prediction"</span><span class="o">,</span> <span class="mf">1.0</span><span class="o">))</span></code></pre></figure>

<p>The result is</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">for balanced data
test accuracy with pipeline 0.9363957597173145
test recall for 1.0 is 0.9444444444444444</code></pre></figure>

<p>As you can observe from the result, our recall has improved a lot. It was 61% percent when data was unbalanced but now it’s 93%. This means our model is pretty good identifying the fraud.</p>

<p>Accuracy score has gone down because we undersampled data. This is fine in our case because if we miss classify some non-fraud transactions as fraud it doesn’t do any harm.</p>

<h2 id="generalisation">Generalisation</h2>

<p>Whenever we undersample data, the training data size reduces significantly. So even though model works well for balanced data, we need to make sure does it generalise well. As we did with python code,the below code calculates score for full data using above model</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">println</span><span class="o">(</span><span class="s">"balanced model for full data"</span><span class="o">)</span>
<span class="nf">printScores</span><span class="o">(</span><span class="n">balancedModel</span><span class="o">,</span> <span class="n">df</span><span class="o">)</span></code></pre></figure>

<p>The result is</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">balanced model for full data
test accuracy with pipeline 0.9439304511476193
test recall for 1.0 is 0.9410569105691057</code></pre></figure>

<p>As you can observe from the results, accuracy score is still good for when we predict for unbalanced data. This makes sure that our model generalises well even if it’s trained on undersample data.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we understood how to implement undersampling technique in spark.</p>


</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">17 Oct 2022</span>
     &raquo; <a href="/latest-java-4">Latest Java Features from a Scala Dev Perspective - Part 4: Higher Order Functions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">10 Oct 2022</span>
     &raquo; <a href="/latest-java-3">Latest Java Features from a Scala Dev Perspective - Part 3: Functional Interfaces</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">14 Sep 2022</span>
     &raquo; <a href="/latest-java-2">Latest Java Features from a Scala Dev Perspective - Part 2: Lambda Expressions</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">21 Jul 2021</span>
     &raquo; <a href="/spark-pandas-part-2">Pandas API on Apache Spark - Part 2: Hello World</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">21 Jul 2021</span>
     &raquo; <a href="/spark-pandas-part-1">Pandas API on Apache Spark - Part 1: Introduction</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">11 Nov 2020</span>
     &raquo; <a href="/barrier-execution-mode-part-2">Barrier Execution Mode in Spark 3.0 - Part 2 : Barrier RDD</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52311191-1', 'auto');
  ga('send', 'pageview');

</script>

    </body>
</html>