<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Extending Spark API</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Thoughts on technology, life and everything else.">
    <link rel="canonical" href="https://blog.madhukaraphatak.com/extending-spark-api">
     <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/app.css">
       
</head>


    <body>

    <header >
  <div class="wrap">
    <a class="site-title" href="/">Madhukar's Blog</a>  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">        
          <a class="page-link" href="http://www.madhukaraphatak.com">About me</a>                  
      </div>
    </nav>  
  </div>
</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
 <header class="post-header">
  <h1>Extending Spark API</h1>
  <p class="meta">Mar 11, 2015</p>
  <div class="catagories">
    
    <a href="/categories/spark"><span class="category">spark</span></a>
    
    <a href="/categories/scala"><span class="category">scala</span></a>
    
  </div>
</header> 

<article class="post-content">
  <p>Apache Spark comes with lot of built in generic operators to do data processing. But many a times, when we are building real world applications, we need domain specific operators to solve problem in hand. So in these cases, we like to extend the Spark API to add our own custom operators.</p>

<p>We can extend spark API in two ways. One of the way is to add custom operator for existing RDD’s and second is to one create our own RDD.</p>

<p>In this post, we are going to discuss both the methods.</p>

<p>tl;dr Access complete code on <a href="https://github.com/phatak-dev/blog/tree/master/code/ExtendingSpark">github</a>.</p>

<h2 id="motivation">Motivation</h2>

<p>Let’ say we have sales data from an online store. The data is in csv format. It contains <em>transactionId</em>, <em>customerId</em>, <em>itemId</em> and <em>itemValue</em>. This model is represented as SalesRecord.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"> <span class="k">class</span> <span class="nc">SalesRecord</span><span class="o">(</span><span class="k">val</span> <span class="nv">transactionId</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                  <span class="k">val</span> <span class="nv">customerId</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                  <span class="k">val</span> <span class="nv">itemId</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                  <span class="k">val</span> <span class="nv">itemValue</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Comparable</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">]</span>
<span class="k">with</span> <span class="nc">Serializable</span></code></pre></figure>

<p>So whenever we get sales data, we convert the raw data to RDD[SalesRecord].</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="nf">args</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="s">"extendingspark"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">dataRDD</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="nf">args</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="nv">salesRecordRDD</span> <span class="k">=</span> <span class="nv">dataRDD</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">row</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="nv">colValues</span> <span class="k">=</span> <span class="nv">row</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">","</span><span class="o">)</span>
    <span class="k">new</span> <span class="nc">SalesRecord</span><span class="o">(</span><span class="nf">colValues</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span><span class="nf">colValues</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span>
    <span class="nf">colValues</span><span class="o">(</span><span class="mi">2</span><span class="o">),</span><span class="nf">colValues</span><span class="o">(</span><span class="mi">3</span><span class="o">).</span><span class="py">toDouble</span><span class="o">)</span>
<span class="o">})</span>  </code></pre></figure>

<p>Let’s say we want to find out total amount of sales, then in Spark we can write</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala">  <span class="nv">salesRecordRDD</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">itemValue</span><span class="o">).</span><span class="py">sum</span>  </code></pre></figure>

<p>though it’s concise, it’s not super readable. It will be nice to have</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nv">salesRecordRDD</span><span class="o">.</span><span class="py">totalSales</span></code></pre></figure>

<p>In the above code, the <em>totalSales</em> feels like built in spark operator.Of course spark don’t know anything about our data or our data model.  Then how we can add our own custom operator on RDD?</p>

<h2 id="adding-custom-operators-to-rdd">Adding custom operators to RDD</h2>

<p>The following are the steps to add custom operator’s to RDD.</p>

<h3 id="step-1--define-utility-class-to-hold-custom-operators">Step 1 : Define Utility class to hold custom operators</h3>

<p>The following code defines an utility class, <em>CustomFunctions</em> , which holds all the custom operators. We take specific RDD,i.e RDD[SalesRecord] so that these operators only available on sales record RDD.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"> 
<span class="k">class</span> <span class="nc">CustomFunctions</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">])</span> <span class="o">{</span>
  <span class="k">def</span> <span class="nf">totalSales</span> <span class="k">=</span> <span class="nv">rdd</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">itemValue</span><span class="o">).</span><span class="py">sum</span>  
<span class="o">}</span>  </code></pre></figure>

<h3 id="step-2--implicit-conversion-to-add-operators-on-rdd">Step 2 : Implicit conversion to add operators on RDD</h3>

<p>The following code defines an implicit function, <em>addCustomFunctions</em> which will add all the custom functions defined in <em>CustomFunctions</em> to the RDD[SalesRecord]</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">object</span> <span class="nc">CustomFunctions</span> <span class="o">{</span>
  <span class="k">implicit</span> <span class="k">def</span> <span class="nf">addCustomFunctions</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">])</span> <span class="k">=</span> <span class="k">new</span>
  <span class="nc">CustomFunctions</span><span class="o">(</span><span class="n">rdd</span><span class="o">)</span> 
<span class="o">}</span>  </code></pre></figure>

<h3 id="step-3-use-custom-functions-using-implicit-import">Step 3: Use custom functions, using implicit import</h3>
<p>The following code has access to custom operator, <em>totalSales</em> using <em>CustomFunctions._</em> import.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">CustomFunctions._</span>
<span class="nf">println</span><span class="o">(</span><span class="nv">salesRecordRDD</span><span class="o">.</span><span class="py">totalSales</span><span class="o">)</span></code></pre></figure>

<p>With the above steps, you defined a domain specific operator on RDD.</p>

<h2 id="creating-custom-rdd">Creating custom RDD</h2>

<p>In the earlier example, we implemented an action which result in single value. But what about the situation where we want to represent lazily evaluated actions?. For example, let’s say we want to give discount to each sales in the RDD. These discounts are lazy in nature. So we need a RDD which can represent the laziness. In following steps we are going to create a RDD called <em>DiscountRDD</em> which holds the discount calculation.</p>

<h3 id="step-1-create-discountrdd-by-extending-rdd">Step 1: Create DiscountRDD by extending RDD</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">DiscountRDD</span><span class="o">(</span><span class="n">prev</span><span class="k">:</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">],</span><span class="n">discountPercentage</span><span class="k">:</span><span class="kt">Double</span><span class="o">)</span> 
<span class="k">extends</span> <span class="nc">RDD</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">](</span><span class="n">prev</span><span class="o">){</span>

<span class="c1">// override compute method to calculate the discount</span>
<span class="k">override</span> <span class="k">def</span> <span class="nf">compute</span><span class="o">(</span><span class="n">split</span><span class="k">:</span> <span class="kt">Partition</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">TaskContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">]</span> <span class="k">=</span>  <span class="o">{</span>
  <span class="n">firstParent</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">].</span><span class="py">iterator</span><span class="o">(</span><span class="n">split</span><span class="o">,</span> <span class="n">context</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="n">salesRecord</span> <span class="k">=&gt;</span> <span class="o">{</span>
      <span class="k">val</span> <span class="nv">discount</span> <span class="k">=</span> <span class="nv">salesRecord</span><span class="o">.</span><span class="py">itemValue</span><span class="o">*</span><span class="n">discountPercentage</span>
      <span class="k">new</span> <span class="nc">SalesRecord</span><span class="o">(</span><span class="nv">salesRecord</span><span class="o">.</span><span class="py">transactionId</span><span class="o">,</span>
      <span class="nv">salesRecord</span><span class="o">.</span><span class="py">customerId</span><span class="o">,</span><span class="nv">salesRecord</span><span class="o">.</span><span class="py">itemId</span><span class="o">,</span><span class="n">discount</span><span class="o">)</span>
<span class="o">})}</span>

<span class="k">override</span> <span class="k">protected</span> <span class="k">def</span> <span class="nf">getPartitions</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Partition</span><span class="o">]</span> <span class="k">=</span> 
<span class="n">firstParent</span><span class="o">[</span><span class="kt">SalesRecord</span><span class="o">].</span><span class="py">partitions</span>
<span class="o">}</span>  </code></pre></figure>

<p>In the above code, we created a RDD called DiscountRDD. It is a RDD derived by applying discount on sales RDD. When we extend RDD, we have to override two methods</p>

<ul>
  <li>####compute</li>
</ul>

<p>This method is the one which computes value for each partition of RDD. In our code, we take input sales record and output it by applying discount as specified by <em>discountPercentage</em>.</p>

<ul>
  <li>####getPartitions</li>
</ul>

<p><em>getPartitions</em> method allows developer to specify the new partitions for the RDD. As we don’t change the partitions in our example, we can just reuse the partitions of parent RDD.</p>

<h3 id="step-2-add-a-custom-operator-named-discount">Step 2: Add a custom operator named <em>discount</em></h3>

<p>Using similar trick discussed earlier, we can add custom operator called <em>discount</em> which creates DiscountRDD.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"> <span class="k">def</span> <span class="nf">discount</span><span class="o">(</span><span class="n">discountPercentage</span><span class="k">:</span><span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DiscountRDD</span><span class="o">(</span><span class="n">rdd</span><span class="o">,</span><span class="n">discountPercentage</span><span class="o">)</span></code></pre></figure>

<h3 id="step-3--use-discount-using-implicit-import">Step 3 : Use discount, using implicit import</h3>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"> <span class="k">import</span> <span class="nn">CustomFunctions._</span>
 <span class="k">val</span> <span class="nv">discountRDD</span> <span class="k">=</span> <span class="nv">salesRecordRDD</span><span class="o">.</span><span class="py">discount</span><span class="o">(</span><span class="mf">0.1</span><span class="o">)</span>
 <span class="nf">println</span><span class="o">(</span><span class="nv">discountRDD</span><span class="o">.</span><span class="py">collect</span><span class="o">().</span><span class="py">toList</span><span class="o">)</span> </code></pre></figure>

<p>So now you know how you can extends spark API for your own domain specific use cases.</p>

</article>
<div class="related">
  <h2>Related posts</h2>
  <ul>
    
             
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">30 May 2023</span>
     &raquo; <a href="/understanding-spark-connect-3">Understanding Spark Connect API - Part 3: Scala API Example</a>    
   </li>           
         

   
   
             
    
    <li>    
     <span class="post-date">17 Jan 2025</span>
     &raquo; <a href="/rediscovering-implicits-scala-3-part-1">Rediscovering Implicits in Scala 3 - Part 1: Implicit Parameters</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">18 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-5">Understanding Spark Connect API - Part 5: Dataframe Sharing Across Spark Sessions</a>    
   </li>           
         

            
    
    <li>    
     <span class="post-date">16 Aug 2023</span>
     &raquo; <a href="/understanding-spark-connect-4">Understanding Spark Connect API - Part 4: PySpark Example</a>    
   </li>           
         

   
   
 </ul>


 
<!--   
</div> -->

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">   
    <div class="footer-col-1 column">
      <ul>
        <li>
          <a href="https://github.com/phatak-dev">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">phatak-dev</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/madhukaraphatak">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">madhukaraphatak</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Thoughts on technology, life and everything else.</p>
    </div>

    <div style="float:right;">
      <a href="/feed.xml"><img src="/images/rss.png">
    </div>



  </div>

</footer>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0ZF0EGSMTQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0ZF0EGSMTQ');
</script>


    </body>
</html>